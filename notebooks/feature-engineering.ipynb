{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d385a31",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d218836",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load IP to Country mapping\n",
    "ip_df = pd.read_csv('../data/raw/IpAddress_to_Country.csv')\n",
    "print(f\"IP mapping shape: {ip_df.shape}\")\n",
    "\n",
    "# Convert IP to integer for range lookup\n",
    "def ip_to_int(ip):\n",
    "    try:\n",
    "        return int(ipaddress.ip_address(ip))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df['ip_int'] = df['ipaddress'].apply(ip_to_int)\n",
    "\n",
    "# Function to find country from IP range\n",
    "def find_country(ip_int):\n",
    "    if pd.isna(ip_int):\n",
    "        return 'Unknown'\n",
    "    country = ip_df[(ip_df['lower_bound_ip_address'] <= ip_int) & \n",
    "                    (ip_df['upper_bound_ip_address'] >= ip_int)]\n",
    "    if not country.empty:\n",
    "        return country.iloc[0]['country']\n",
    "    return 'Unknown'\n",
    "\n",
    "df['country'] = df['ip_int'].apply(find_country)\n",
    "print(f\"Countries mapped: {df['country'].nunique()}\")\n",
    "\n",
    "# Analyze fraud by country\n",
    "country_fraud = df.groupby('country')['class'].agg(['count', 'mean', 'sum'])\n",
    "country_fraud = country_fraud.sort_values('mean', ascending=False)\n",
    "print(\"\\nTop 10 High Fraud Rate Countries:\")\n",
    "print(country_fraud.head(10))\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_20 = country_fraud.head(20)\n",
    "plt.barh(top_20.index, top_20['mean']*100, color='crimson')\n",
    "plt.xlabel('Fraud Rate (%)')\n",
    "plt.title('Top 20 Countries by Fraud Rate')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/fraud_by_country.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae550f1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Time-based features\n",
    "df['hour_of_day'] = df['purchasetime'].dt.hour\n",
    "df['day_of_week'] = df['purchasetime'].dt.dayofweek\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# 2. Time since signup (CRITICAL FEATURE)\n",
    "df['time_since_signup'] = (df['purchasetime'] - df['signuptime']).dt.total_seconds() / 3600  # in hours\n",
    "\n",
    "# 3. Transaction frequency features\n",
    "# Sort by user and time\n",
    "df = df.sort_values(['userid', 'purchasetime'])\n",
    "\n",
    "# Calculate time since last transaction per user\n",
    "df['time_since_last_txn'] = df.groupby('userid')['purchasetime'].diff().dt.total_seconds() / 60  # minutes\n",
    "\n",
    "# Transaction count in last 24h, 7d (using expanding window)\n",
    "df['txn_count_24h'] = df.groupby('userid')['purchasetime'].transform(\n",
    "    lambda x: x.rolling('24h', on=x).count()\n",
    ")\n",
    "df['txn_count_7d'] = df.groupby('userid')['purchasetime'].transform(\n",
    "    lambda x: x.rolling('7d', on=x).count()\n",
    ")\n",
    "\n",
    "# 4. Velocity features\n",
    "df['purchase_velocity_1h'] = df.groupby('userid')['purchasevalue'].transform(\n",
    "    lambda x: x.rolling('1h', on=df.loc[x.index, 'purchasetime']).sum()\n",
    ")\n",
    "df['avg_purchase_user'] = df.groupby('userid')['purchasevalue'].transform('mean')\n",
    "\n",
    "# 5. Device usage features\n",
    "df['unique_devices_per_user'] = df.groupby('userid')['deviceid'].transform('nunique')\n",
    "\n",
    "print(f\"Total features created: {len(df.columns)}\")\n",
    "print(f\"New features: hour_of_day, day_of_week, is_weekend, time_since_signup, time_since_last_txn, txn_count_24h, txn_count_7d, purchase_velocity_1h, avg_purchase_user, unique_devices_per_user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37efd96c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('class', axis=1)\n",
    "y = df['class']\n",
    "\n",
    "# Identify column types\n",
    "categorical_cols = ['source', 'browser', 'sex', 'country', 'deviceid']\n",
    "numerical_cols = ['purchasevalue', 'age', 'time_since_signup', 'time_since_last_txn',\n",
    "                  'txn_count_24h', 'txn_count_7d', 'purchase_velocity_1h',\n",
    "                  'avg_purchase_user', 'unique_devices_per_user']\n",
    "\n",
    "# Remove identifiers and datetime from modeling\n",
    "exclude_cols = ['userid', 'signuptime', 'purchasetime', 'ipaddress', 'ip_int', 'purchase_hour']\n",
    "X_model = X.drop(columns=exclude_cols, errors='ignore')\n",
    "\n",
    "# Update column lists after dropping\n",
    "categorical_cols = [col for col in categorical_cols if col in X_model.columns]\n",
    "numerical_cols = [col for col in numerical_cols if col in X_model.columns]\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Fit and transform\n",
    "X_processed = preprocessor.fit_transform(X_model)\n",
    "print(f\"Processed shape: {X_processed.shape}\")\n",
    "\n",
    "# Get feature names\n",
    "cat_encoder = preprocessor.named_transformers_['cat']\n",
    "cat_features = cat_encoder.get_feature_names_out(categorical_cols)\n",
    "all_features = list(numerical_cols) + list(cat_features)\n",
    "print(f\"Total features after encoding: {len(all_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99c61ab",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train-test split FIRST (IMPORTANT: never apply SMOTE before splitting!)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CLASS IMBALANCE HANDLING STRATEGY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBefore resampling:\")\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Fraud cases in training: {y_train.sum()} ({y_train.mean()*100:.2f}%)\")\n",
    "print(f\"Non-fraud cases in training: {len(y_train)-y_train.sum()}\")\n",
    "\n",
    "# DECISION: Use SMOTE for moderate oversampling + RandomUnderSampling for majority\n",
    "# Justification written in markdown cell:\n",
    "\"\"\"\n",
    "## Justification for Imbalance Handling Strategy\n",
    "\n",
    "**Why SMOTE?**\n",
    "- Our fraud rate is extremely low (~1-2% typical for fraud data)\n",
    "- SMOTE creates synthetic minority samples instead of duplicating\n",
    "- Helps prevent overfitting compared to random oversampling\n",
    "- Preserves the information in the majority class\n",
    "\n",
    "**Why combine with RandomUnderSampling?**\n",
    "- Pure SMOTE on severe imbalance can lead to overgeneralization\n",
    "- Under-sampling majority reduces computational cost\n",
    "- Combination (SMOTEENN or pipeline) often gives better decision boundaries\n",
    "\n",
    "**Alternative considered:**\n",
    "- Class weighting in models: Good, but doesn't create new patterns\n",
    "- ADASYN: Similar to SMOTE but focuses on difficult samples\n",
    "\n",
    "**Final decision:** SMOTE to 10% fraud rate, then light under-sampling\n",
    "\"\"\"\n",
    "\n",
    "# Apply SMOTE (oversample minority)\n",
    "smote = SMOTE(sampling_strategy=0.1, random_state=42)  # Target 10% fraud rate\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"\\nAfter SMOTE (10% fraud rate target):\")\n",
    "print(f\"Training set shape: {X_train_smote.shape}\")\n",
    "print(f\"Fraud cases: {y_train_smote.sum()} ({y_train_smote.mean()*100:.2f}%)\")\n",
    "\n",
    "# Optional: Light under-sampling of majority\n",
    "rus = RandomUnderSampler(sampling_strategy=0.5, random_state=42)  # 2:1 ratio\n",
    "X_train_balanced, y_train_balanced = rus.fit_resample(X_train_smote, y_train_smote)\n",
    "\n",
    "print(f\"\\nAfter final balancing:\")\n",
    "print(f\"Training set shape: {X_train_balanced.shape}\")\n",
    "print(f\"Fraud rate: {y_train_balanced.mean()*100:.2f}%\")\n",
    "print(f\"Class distribution: {pd.Series(y_train_balanced).value_counts().to_dict()}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Before\n",
    "axes[0].pie([len(y_train)-y_train.sum(), y_train.sum()], \n",
    "            labels=['Non-Fraud', 'Fraud'], autopct='%1.1f%%',\n",
    "            colors=['#3498db', '#e74c3c'])\n",
    "axes[0].set_title('Before Resampling')\n",
    "\n",
    "# After SMOTE\n",
    "axes[1].pie([len(y_train_smote)-y_train_smote.sum(), y_train_smote.sum()],\n",
    "            labels=['Non-Fraud', 'Fraud'], autopct='%1.1f%%',\n",
    "            colors=['#3498db', '#e74c3c'])\n",
    "axes[1].set_title('After SMOTE (10%)')\n",
    "\n",
    "# Final\n",
    "axes[2].pie([len(y_train_balanced)-y_train_balanced.sum(), y_train_balanced.sum()],\n",
    "            labels=['Non-Fraud', 'Fraud'], autopct='%1.1f%%',\n",
    "            colors=['#3498db', '#e74c3c'])\n",
    "axes[2].set_title('Final Balanced (2:1)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/class_balance_evolution.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Save processed data\n",
    "processed_data = {\n",
    "    'X_train': X_train_balanced,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train_balanced,\n",
    "    'y_test': y_test,\n",
    "    'feature_names': all_features,\n",
    "    'preprocessor': preprocessor\n",
    "}\n",
    "\n",
    "import joblib\n",
    "joblib.dump(processed_data, '../data/processed/train_test_data.pkl')\n",
    "print(\"\\nâœ“ Processed data saved to ../data/processed/train_test_data.pkl\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
